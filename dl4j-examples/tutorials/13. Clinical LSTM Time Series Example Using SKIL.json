{"paragraphs":[{"text":"%md\n### Note\n\nPlease view the [README](https://github.com/deeplearning4j/dl4j-examples/tree/overhaul_tutorials/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin","user":"admin","dateUpdated":"2018-02-26T03:42:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Note</h3>\n<p>Please view the <a href=\"https://github.com/deeplearning4j/dl4j-examples/tree/overhaul_tutorials/tutorials/README.md\">README</a> to learn about installing, setting up dependencies, and importing notebooks in Zeppelin</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805174_-853451049","id":"20180115-062124_1656190287","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:07+0000","dateFinished":"2018-02-26T03:42:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11159"},{"text":"%md\n### Background","user":"admin","dateUpdated":"2018-02-26T03:42:10+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Background</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805181_-857683287","id":"20180116-040712_1768508310","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:10+0000","dateFinished":"2018-02-26T03:42:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11160"},{"text":"%md\nIn this tutorial, we will learn how to apply a long-short term memory (LSTM) neural network to a medical time series problem. The data used comes from 4000 intensive care unit (ICU) patients and the goal is to predict the mortality of patients using 6 general descriptor features, such as age, gender, and weight along with 37 sequential features, such as cholesterol level, temperature, pH, and glucose level. Each patient has multiple measurements of the sequential features, with patients having a different amount of measurements taken. Furthermore, the time between measurements also differ among patients as well. \n\nA LSTM is well suited for this type of problem due to the sequential nature of the data. In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks. For a more in depth explanation of LSTM's, see https://deeplearning4j.org/lstm.html.","user":"admin","dateUpdated":"2018-02-26T03:42:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In this tutorial, we will learn how to apply a long-short term memory (LSTM) neural network to a medical time series problem. The data used comes from 4000 intensive care unit (ICU) patients and the goal is to predict the mortality of patients using 6 general descriptor features, such as age, gender, and weight along with 37 sequential features, such as cholesterol level, temperature, pH, and glucose level. Each patient has multiple measurements of the sequential features, with patients having a different amount of measurements taken. Furthermore, the time between measurements also differ among patients as well.</p>\n<p>A LSTM is well suited for this type of problem due to the sequential nature of the data. In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks. For a more in depth explanation of LSTM's, see https://deeplearning4j.org/lstm.html.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805182_-856529040","id":"20180116-040741_1071141793","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:11+0000","dateFinished":"2018-02-26T03:42:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11161"},{"text":"%md\n### Imports","user":"admin","dateUpdated":"2018-02-26T03:42:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Imports</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805182_-856529040","id":"20180115-062155_892306848","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:14+0000","dateFinished":"2018-02-26T03:42:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11162"},{"text":"import io.skymind.zeppelin.utils._\nimport io.skymind.modelproviders.history.client.ModelHistoryClient\nimport io.skymind.modelproviders.history.model._\nimport org.datavec.api.records.reader.SequenceRecordReader;\nimport org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader;\nimport org.datavec.api.split.NumberedFileInputSplit;\nimport org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator;\nimport org.deeplearning4j.eval.ROC;\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm;\nimport org.deeplearning4j.nn.conf.ComputationGraphConfiguration;\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration;\nimport org.deeplearning4j.nn.conf.Updater;\nimport org.deeplearning4j.nn.conf.layers.GravesLSTM;\nimport org.deeplearning4j.nn.conf.layers.RnnOutputLayer;\nimport org.deeplearning4j.nn.graph.ComputationGraph;\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener;\nimport org.deeplearning4j.nn.conf.graph.rnn.LastTimeStepVertex;\nimport org.deeplearning4j.nn.conf.layers.OutputLayer;\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.deeplearning4j.nn.weights.WeightInit;\nimport org.nd4j.linalg.activations.Activation;\nimport org.nd4j.linalg.dataset.api.DataSet;\nimport org.nd4j.linalg.lossfunctions.LossFunctions;\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\nimport org.nd4j.linalg.dataset.api.DataSetPreProcessor;\nimport org.nd4j.linalg.factory.Nd4j;\nimport org.nd4j.linalg.indexing.BooleanIndexing;\nimport org.nd4j.linalg.indexing.NDArrayIndex;\nimport org.nd4j.linalg.indexing.conditions.Conditions;\nimport org.nd4j.linalg.primitives.Pair;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport org.apache.commons.io.FileUtils;\nimport org.apache.commons.io.FilenameUtils;\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Arrays;\nimport java.net.URL;\nimport java.io.BufferedInputStream;\nimport java.io.FileInputStream;\nimport java.io.BufferedOutputStream;\nimport java.io.FileOutputStream;\nimport java.lang.Byte;\n\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry;\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\n\n\n","user":"admin","dateUpdated":"2018-02-13T00:26:56+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import io.skymind.zeppelin.utils._\nimport io.skymind.modelproviders.history.client.ModelHistoryClient\nimport io.skymind.modelproviders.history.model._\nimport org.datavec.api.records.reader.SequenceRecordReader\nimport org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader\nimport org.datavec.api.split.NumberedFileInputSplit\nimport org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator\nimport org.deeplearning4j.eval.ROC\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.ComputationGraphConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.GravesLSTM\nimport org.deeplearning4j.nn.conf.layers.RnnOutputLayer\nimport org.deeplearning4j.nn.graph.ComputationGraph\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener\nimport org.deeplearning4j.nn.conf.graph.rnn.LastTimeStepVertex\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.nd4j.linalg.api.ndarray.INDArray\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.dataset.api.DataSet\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.dataset.api.DataSetPreProcessor\nimport org.nd4j.linalg.factory.Nd4j\nimport org.nd4j.linalg.indexing.BooleanIndexing\nimport org.nd4j.linalg.indexing.NDArrayIndex\nimport org.nd4j.linalg.indexing.conditions.Conditions\nimport org.nd4j.linalg.primitives.Pair\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport java.io.File\nimport org.apache.commons.io.FileUtils\nimport org.apache.commons.io.FilenameUtils\nimport java.io.IOException\nimport java.util.HashMap\nimport java.util.Arrays\nimport java.net.URL\nimport java.io.BufferedInputStream\nimport java.io.FileInputStream\nimport java.io.BufferedOutputStream\nimport java.io.FileOutputStream\nimport java.lang.Byte\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream\n"}]},"apps":[],"jobName":"paragraph_1516763805182_-856529040","id":"20180115-062209_1381660770","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:26:56+0000","dateFinished":"2018-02-13T00:27:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11163"},{"text":"%md\n\nNow that we have imported everything needed to run this tutorial, we will start with obtaining the data and then converting the  data into a format a neural network can understand. ","user":"admin","dateUpdated":"2018-02-26T03:42:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that we have imported everything needed to run this tutorial, we will start with obtaining the data and then converting the  data into a format a neural network can understand.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805182_-856529040","id":"20180116-041926_21274632","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:23+0000","dateFinished":"2018-02-26T03:42:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11164"},{"text":"%md\n### Data Source","user":"admin","dateUpdated":"2018-02-26T03:42:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Data Source</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805182_-856913789","id":"20180115-063620_1089188478","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:25+0000","dateFinished":"2018-02-26T03:42:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11165"},{"text":"%md\nThe data is contained in a compressed tar.gz file. We will have to download the data from the url below and then extract csv files containing the ICU data. Each patient will have a separate csv file for the features and labels. The features will be contained in a directory called sequence and the labels will be contained in a directory called mortality. The features are contained in a single csv file with the columns representing the features and the rows representing different time steps. The labels are contained in a single csv file which contains a value of 0 indicating death and a value of 1 indicating survival.","user":"admin","dateUpdated":"2018-02-26T03:42:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The data is contained in a compressed tar.gz file. We will have to download the data from the url below and then extract csv files containing the ICU data. Each patient will have a separate csv file for the features and labels. The features will be contained in a directory called sequence and the labels will be contained in a directory called mortality. The features are contained in a single csv file with the columns representing the features and the rows representing different time steps. The labels are contained in a single csv file which contains a value of 0 indicating death and a value of 1 indicating survival.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805183_-856913789","id":"20180116-042022_82417057","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:26+0000","dateFinished":"2018-02-26T03:42:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11166"},{"text":"val DATA_URL = \"https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz\"\nval DATA_PATH = FilenameUtils.concat(System.getProperty(\"java.io.tmpdir\"), \"dl4j_physionet/\")","user":"admin","dateUpdated":"2018-02-13T00:27:02+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DATA_URL: String = https://skymindacademy.blob.core.windows.net/physionet2012/physionet2012.tar.gz\nDATA_PATH: String = /tmp/dl4j_physionet/\n"}]},"apps":[],"jobName":"paragraph_1516763805183_-856913789","id":"20180115-063627_2071788954","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:02+0000","dateFinished":"2018-02-13T00:27:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11167"},{"text":"%md\n### Download Data","user":"admin","dateUpdated":"2018-02-26T03:42:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Download Data</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805183_-1166636654","id":"20180115-063817_834797484","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:29+0000","dateFinished":"2018-02-26T03:42:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11168"},{"text":"%md\nTo download the data, we will create a temporary directory that will store the data files, extract the tar.gz file from the url, and place it in the specified directory.","user":"admin","dateUpdated":"2018-02-26T03:42:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>To download the data, we will create a temporary directory that will store the data files, extract the tar.gz file from the url, and place it in the specified directory.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805184_-1166636654","id":"20180116-042108_1540744847","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:30+0000","dateFinished":"2018-02-26T03:42:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11169"},{"text":"val directory = new File(DATA_PATH)\ndirectory.mkdir() // create new directory at specified path\n\nval archizePath = DATA_PATH + \"physionet2012.tar.gz\" // set path for tar.gz file\nval archiveFile = new File(archizePath) // create tar.gz file\nval extractedPath = DATA_PATH + \"physionet2012\" \nval extractedFile = new File(extractedPath)\n\nFileUtils.copyURLToFile(new URL(DATA_URL), archiveFile) // copy data from URL to file","user":"admin","dateUpdated":"2018-02-13T00:27:05+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"directory: java.io.File = /tmp/dl4j_physionet\nres84: Boolean = false\narchizePath: String = /tmp/dl4j_physionet/physionet2012.tar.gz\narchiveFile: java.io.File = /tmp/dl4j_physionet/physionet2012.tar.gz\nextractedPath: String = /tmp/dl4j_physionet/physionet2012\nextractedFile: java.io.File = /tmp/dl4j_physionet/physionet2012\n"}]},"apps":[],"jobName":"paragraph_1516763805184_-1166636654","id":"20180115-063829_1644657611","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:05+0000","dateFinished":"2018-02-13T00:27:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11170"},{"text":"%md \n\nNext, we must extract the data from the tar.gz file, recreate directories within the tar.gz file into our temporary directory, and copy the files into our temporary directory. ","user":"admin","dateUpdated":"2018-02-26T03:42:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, we must extract the data from the tar.gz file, recreate directories within the tar.gz file into our temporary directory, and copy the files into our temporary directory.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805184_-1166636654","id":"20180115-064020_988667885","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:32+0000","dateFinished":"2018-02-26T03:42:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11171"},{"text":"var fileCount = 0\nvar dirCount = 0\nval BUFFER_SIZE = 4096\n\nval tais = new TarArchiveInputStream(new GzipCompressorInputStream( new BufferedInputStream( new FileInputStream(archizePath))))\n\nvar entry = tais.getNextEntry().asInstanceOf[TarArchiveEntry]\n\nwhile(entry != null){\n    if (entry.isDirectory()) {\n        new File(DATA_PATH + entry.getName()).mkdirs()\n        dirCount = dirCount + 1\n        fileCount = 0\n    }\n    else {\n        \n        val data = new Array[scala.Byte](4 * BUFFER_SIZE)\n\n        val fos = new FileOutputStream(DATA_PATH + entry.getName());\n        val dest = new BufferedOutputStream(fos, BUFFER_SIZE);\n        var count = tais.read(data, 0, BUFFER_SIZE)\n        \n        while (count != -1) {\n            dest.write(data, 0, count)\n            count = tais.read(data, 0, BUFFER_SIZE)\n        }\n        \n        dest.close()\n        fileCount = fileCount + 1\n    }\n    if(fileCount % 1000 == 0){\n        print(\".\")\n    }\n    \n    entry = tais.getNextEntry().asInstanceOf[TarArchiveEntry]\n}","user":"admin","dateUpdated":"2018-02-13T00:27:07+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"fileCount: Int = 0\ndirCount: Int = 0\nBUFFER_SIZE: Int = 4096\ntais: org.apache.commons.compress.archivers.tar.TarArchiveInputStream = org.apache.commons.compress.archivers.tar.TarArchiveInputStream@63e3150a\nentry: org.apache.commons.compress.archivers.tar.TarArchiveEntry = org.apache.commons.compress.archivers.tar.TarArchiveEntry@221cf6b\n........................................."}]},"apps":[],"jobName":"paragraph_1516763805184_-1166636654","id":"20180115-070134_320419844","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:07+0000","dateFinished":"2018-02-13T00:27:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11172"},{"text":"%md\n### DataSetIterators","user":"admin","dateUpdated":"2018-02-26T03:42:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>DataSetIterators</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805185_-1167021402","id":"20180115-070220_1700945870","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:35+0000","dateFinished":"2018-02-26T03:42:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11173"},{"text":"%md\nOur next goal is to convert the raw data (csv files) into a DataSetIterator, which can then be fed into a neural network for training. Our training data will have 3200 examples which will be represented by a single DataSetIterator, and the testing data will have 800 examples which will be represented by a separate DataSet Iterator.","user":"admin","dateUpdated":"2018-02-26T03:42:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Our next goal is to convert the raw data (csv files) into a DataSetIterator, which can then be fed into a neural network for training. Our training data will have 3200 examples which will be represented by a single DataSetIterator, and the testing data will have 800 examples which will be represented by a separate DataSet Iterator.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805185_-1167021402","id":"20180116-043157_768466617","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:37+0000","dateFinished":"2018-02-26T03:42:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11174"},{"text":"val NB_TRAIN_EXAMPLES = 2000 // number of training examples\nval NB_TEST_EXAMPLES = 800 // number of testing examples","user":"admin","dateUpdated":"2018-02-13T00:27:10+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"NB_TRAIN_EXAMPLES: Int = 2000\nNB_TEST_EXAMPLES: Int = 800\n"}]},"apps":[],"jobName":"paragraph_1516763805185_-1167021402","id":"20180115-071757_4986863","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:10+0000","dateFinished":"2018-02-13T00:27:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11175"},{"text":"%md\nIn order to obtain DataSetIterators, we must first initialize CSVSequenceRecordReaders, which will parse the raw data into record-like format. We will first set the directories for the features and labels and initialize the CSVSequenceRecordReaders.\n\nNext, we can initialize the SequenceRecordReaderDataSetIterator using the previously created CSVSequenceRecordReaders. We will use an alignment mode of ALIGN_END. This alignment mode is needed due to the fact that the number of time steps differs between different patients. Because the mortality label is always at the end of the sequence, we need all the sequences aligned so that the time step with the mortality label is the last time step for all patients. For a more in depth explanation of alignment modes, see https://deeplearning4j.org/usingrnns. ","user":"admin","dateUpdated":"2018-02-26T03:42:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In order to obtain DataSetIterators, we must first initialize CSVSequenceRecordReaders, which will parse the raw data into record-like format. We will first set the directories for the features and labels and initialize the CSVSequenceRecordReaders.</p>\n<p>Next, we can initialize the SequenceRecordReaderDataSetIterator using the previously created CSVSequenceRecordReaders. We will use an alignment mode of ALIGN_END. This alignment mode is needed due to the fact that the number of time steps differs between different patients. Because the mortality label is always at the end of the sequence, we need all the sequences aligned so that the time step with the mortality label is the last time step for all patients. For a more in depth explanation of alignment modes, see https://deeplearning4j.org/usingrnns.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805185_-1167021402","id":"20180115-070711_769734985","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:39+0000","dateFinished":"2018-02-26T03:42:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11176"},{"text":"val path = FilenameUtils.concat(DATA_PATH, \"physionet2012/\") // set parent directory\n\nval featureBaseDir = FilenameUtils.concat(path, \"sequence\") // set feature directory\nval mortalityBaseDir = FilenameUtils.concat(path, \"mortality\") // set label directory\n\n// Load training data\n\nval trainFeatures = new CSVSequenceRecordReader(1, \",\")\ntrainFeatures.initialize( new NumberedFileInputSplit(featureBaseDir + \"/%d.csv\", 0, NB_TRAIN_EXAMPLES - 1))\n\nval trainLabels = new CSVSequenceRecordReader()\ntrainLabels.initialize(new NumberedFileInputSplit(mortalityBaseDir + \"/%d.csv\", 0, NB_TRAIN_EXAMPLES - 1))\n\nval trainData = new SequenceRecordReaderDataSetIterator(trainFeatures, trainLabels,\n                1, 2, false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END)\n\n        \n// Load testing data\nval testFeatures = new CSVSequenceRecordReader(1, \",\");\ntestFeatures.initialize(new NumberedFileInputSplit(featureBaseDir + \"/%d.csv\", NB_TRAIN_EXAMPLES, NB_TRAIN_EXAMPLES + 50));\n       \nval testLabels = new CSVSequenceRecordReader();\ntestLabels.initialize(new NumberedFileInputSplit(mortalityBaseDir + \"/%d.csv\", NB_TRAIN_EXAMPLES, NB_TRAIN_EXAMPLES  + 50));\n\nval testData =  new SequenceRecordReaderDataSetIterator(testFeatures, testLabels,\n                1, 2, false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END)\n","user":"admin","dateUpdated":"2018-02-13T00:27:15+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = /tmp/dl4j_physionet/physionet2012/\nfeatureBaseDir: String = /tmp/dl4j_physionet/physionet2012/sequence\nmortalityBaseDir: String = /tmp/dl4j_physionet/physionet2012/mortality\ntrainFeatures: org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader = org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader@11c879f2\ntrainLabels: org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader = org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader@4069ebeb\ntrainData: org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator = org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator@3bc84b06\ntestFeatures: org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader = org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader@22ed2e21\ntestLabels: org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader = org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader@22f5581\ntestData: org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator = org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator@37818958\n"}]},"apps":[],"jobName":"paragraph_1516763805185_-1167021402","id":"20180115-070257_386614995","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:50+0000","dateFinished":"2018-02-13T00:27:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11177"},{"text":"%md\n### Neural Network Configuration","user":"admin","dateUpdated":"2018-02-26T03:42:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Neural Network Configuration</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805186_-1165867156","id":"20180115-062914_1963273192","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:53+0000","dateFinished":"2018-02-26T03:42:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11178"},{"text":"%md\nNow we can finally configure and then initialize the neural network for this problem. We will be using the ComputationGraph class of DL4J.","user":"admin","dateUpdated":"2018-02-26T03:42:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now we can finally configure and then initialize the neural network for this problem. We will be using the ComputationGraph class of DL4J.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805186_-1165867156","id":"20180116-045323_1278918662","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:55+0000","dateFinished":"2018-02-26T03:42:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11179"},{"text":"// Set neural network parameters\nval NB_INPUTS = 86\nval NB_EPOCHS = 10\nval RANDOM_SEED = 1234\nval LEARNING_RATE = 0.005\nval BATCH_SIZE = 32\nval LSTM_LAYER_SIZE = 200\nval NUM_LABEL_CLASSES = 2 ","user":"admin","dateUpdated":"2018-02-13T00:27:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"NB_INPUTS: Int = 86\nNB_EPOCHS: Int = 10\nRANDOM_SEED: Int = 1234\nLEARNING_RATE: Double = 0.005\nBATCH_SIZE: Int = 32\nLSTM_LAYER_SIZE: Int = 200\nNUM_LABEL_CLASSES: Int = 2\n"}]},"apps":[],"jobName":"paragraph_1516763805186_-1165867156","id":"20180115-062305_1051366040","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:27:50+0000","dateFinished":"2018-02-13T00:27:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11180"},{"text":"\nval conf = new NeuralNetConfiguration.Builder()\n                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n                .l2(0.01)\n                .graphBuilder()\n                .addInputs(\"in\")\n                .addLayer(\"lstm\", new GravesLSTM.Builder().nIn(NB_INPUTS).nOut(30).build(), \"in\")\n                .addVertex(\"lastStep\", new LastTimeStepVertex(\"in\"), \"lstm\")\n                .addLayer(\"out\", new OutputLayer.Builder().activation(Activation.SOFTMAX).nIn(30).nOut(2)\n                        .build(), \"lastStep\")\n                .setOutputs(\"out\")\n                .build();\n\n\nval model = new ComputationGraph(conf);\nmodel.init();","user":"admin","dateUpdated":"2018-02-13T01:14:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"conf: org.deeplearning4j.nn.conf.ComputationGraphConfiguration = \n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"defaultConfiguration\" : {\n    \"cacheMode\" : \"NONE\",\n    \"epochCount\" : 0,\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : null,\n    \"maxNumLineSearchIterations\" : 5,\n    \"miniBatch\" : true,\n    \"minimize\" : true,\n    \"optimizationAlgo\" : \"STOCHASTIC_GRADIENT_DESCENT\",\n    \"pretrain\" : false,\n    \"seed\" : 1518481672314,\n    \"stepFunction\" : null,\n    \"variables\" : [ ]\n  },\n  \"epochCount\" : 0,\n  \"inferenceWorkspaceMode\" : \"SEPARATE\",\n  \"iterationCount\" : 0,\n  \"networkInputs\" : [ \"in\" ],\n  \"networkOutputs\" : [ \"out\" ],\n  \"pretrain\" : false,\n  \"tbpttBackLength\" : 20,\n  \"tbpttFwdLength\" : 20,\n  \"trainingWorksp...model: org.deeplearning4j.nn.graph.ComputationGraph = org.deeplearning4j.nn.graph.ComputationGraph@72efb097\n"}]},"apps":[],"jobName":"paragraph_1517629926853_-1196414837","id":"20180203-035206_1227695564","dateCreated":"2018-02-03T03:52:06+0000","dateStarted":"2018-02-13T00:27:51+0000","dateFinished":"2018-02-13T00:27:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11181"},{"text":"%md\n\n### Training","user":"admin","dateUpdated":"2018-02-26T03:42:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Training</h3>\n"}]},"apps":[],"jobName":"paragraph_1516763805186_-1165867156","id":"20180115-200258_1843146082","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:42:58+0000","dateFinished":"2018-02-26T03:42:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11182"},{"text":"%md\nBefore we train the neural network, we first need to preprocess the data so that only the last step of the labels array is used for training. Thus, we will define the LastStepPreProc class to do this, which is an extension of the DataSetPreProcessor.","user":"admin","dateUpdated":"2018-02-26T03:43:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Before we train the neural network, we first need to preprocess the data so that only the last step of the labels array is used for training. Thus, we will define the LastStepPreProc class to do this, which is an extension of the DataSetPreProcessor.</p>\n"}]},"apps":[],"jobName":"paragraph_1516763805187_-1166251905","id":"20180116-045440_285367027","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-26T03:43:00+0000","dateFinished":"2018-02-26T03:43:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11183"},{"text":"class LastStepPreProc extends DataSetPreProcessor {\n\n    override def preProcess(in : DataSet) {\n\n        val origLabels = in.getLabels();\n        val lMask = in.getLabelsMaskArray();\n\n        //On master: use TimeSeriesUtils.pullLastTimeSteps(origLabels, lMask);\n        \n        val labels2d = pullLastTimeSteps(origLabels, lMask);\n      \n        in.setLabels(labels2d);\n        in.setLabelsMaskArray(null);\n    }\n\n     def pullLastTimeSteps( pullFrom : INDArray, mask : INDArray) : INDArray = {\n        if (mask == null) {\n            //No mask array -> extract same (last) column for all\n            var lastTS = pullFrom.size(2) - 1;\n            var out = pullFrom.get(NDArrayIndex.all(), NDArrayIndex.all(), NDArrayIndex.point(lastTS));\n            var fwdPassTimeSteps = null; //Null -> last time step for all examples\n            out\n            \n        } else {\n            var outShape = new Array[Double](2);\n            outShape(0) = pullFrom.size(0);\n            outShape(1) = pullFrom.size(1);\n                \n            var out = Nd4j.create(outShape);\n\n            //Want the index of the last non-zero entry in the mask array\n            var lastStepArr = BooleanIndexing.lastIndex(mask, Conditions.epsNotEquals(0.0), 1);\n            var fwdPassTimeSteps = lastStepArr.data().asInt();\n            \n            for ( i <- 0 to fwdPassTimeSteps.length-1) {\n                out.putRow(i, pullFrom.get(NDArrayIndex.point(i), NDArrayIndex.all(),\n                        NDArrayIndex.point(fwdPassTimeSteps(i))));\n            }\n            out\n        }\n    }\n}","user":"admin","dateUpdated":"2018-02-13T01:14:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class LastStepPreProc\n"}]},"apps":[],"jobName":"paragraph_1518422389054_1063008804","id":"20180212-075949_266693958","dateCreated":"2018-02-12T07:59:49+0000","dateStarted":"2018-02-12T22:25:06+0000","dateFinished":"2018-02-12T22:25:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11184"},{"text":"%md \n\nTo actually train the neural network, we use a for loop for the number of epochs to train. We then extract each DataSet, preprocess it, and fit it to the model.","user":"admin","dateUpdated":"2018-02-26T03:43:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>To actually train the neural network, we use a for loop for the number of epochs to train. We then extract each DataSet, preprocess it, and fit it to the model.</p>\n"}]},"apps":[],"jobName":"paragraph_1518474133050_-841589885","id":"20180212-222213_2027748665","dateCreated":"2018-02-12T22:22:13+0000","dateStarted":"2018-02-26T03:43:04+0000","dateFinished":"2018-02-26T03:43:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11185"},{"text":"val proc = new LastStepPreProc()\n\ntrainData.reset()\n\nfor( i <- 1 to 5){\n    println(\"Epoch:\")\n    println(i)\n    while(trainData.hasNext()){\n        val batch = trainData.next()\n        proc.preProcess(batch) \n        model.fit(batch)\n    }\n    trainData.reset()\n}\n","user":"admin","dateUpdated":"2018-02-13T00:27:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"proc: LastStepPreProc = $iwC$$iwC$LastStepPreProc@4ff32fc5\nEpoch:\n1\n"}]},"apps":[],"jobName":"paragraph_1517630111434_571935436","id":"20180203-035511_669197466","dateCreated":"2018-02-03T03:55:11+0000","dateStarted":"2018-02-13T00:27:52+0000","dateFinished":"2018-02-13T00:32:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11186"},{"text":"%md \n\n### Adding Model to SKIL Experiment\n","user":"admin","dateUpdated":"2018-02-26T03:43:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Adding Model to SKIL Experiment</h3>\n"}]},"apps":[],"jobName":"paragraph_1518469669663_1872975961","id":"20180212-210749_1826023024","dateCreated":"2018-02-12T21:07:49+0000","dateStarted":"2018-02-26T03:43:06+0000","dateFinished":"2018-02-26T03:43:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11187"},{"text":"%md\nTo finally add the model to the SKIL experiment, we will initialize a skilContext and use the addModelToExperiment method.","user":"admin","dateUpdated":"2018-02-26T03:43:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>To finally add the model to the SKIL experiment, we will initialize a skilContext and use the addModelToExperiment method.</p>\n"}]},"apps":[],"jobName":"paragraph_1518482241772_1862408342","id":"20180213-003721_1171320857","dateCreated":"2018-02-13T00:37:21+0000","dateStarted":"2018-02-26T03:43:08+0000","dateFinished":"2018-02-26T03:43:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11188"},{"text":"val skilContext = new SkilContext()\nval client = skilContext.client\nval model_id = skilContext.addModelToExperiment(z, model)\n","user":"admin","dateUpdated":"2018-02-13T00:34:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"skilContext: io.skymind.zeppelin.utils.SkilContext = io.skymind.zeppelin.utils.SkilContext@28fe7273\nclient: io.skymind.modelproviders.history.client.ModelHistoryClient = io.skymind.modelproviders.history.client.ModelHistoryClient@1dd55dfc\nmodel_id: String = 5cfb4de2-aa44-4d7b-8b36-851d7a4995aa\n"}]},"apps":[],"jobName":"paragraph_1516763805188_-1168175649","id":"20180116-045740_857601022","dateCreated":"2018-01-24T03:16:45+0000","dateStarted":"2018-02-13T00:34:41+0000","dateFinished":"2018-02-13T00:34:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11189"},{"user":"admin","dateUpdated":"2018-02-09T03:56:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1517994179681_-540994179","id":"20180207-090259_714890604","dateCreated":"2018-02-07T09:02:59+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:11190"}],"name":"ClinicalTimeSeriesLSTMExample","id":"2D4MNUD8E","angularObjects":{"2D9577PF1:existing_process":[],"2D86VT151:existing_process":[],"2D7PUMBZ1:existing_process":[],"2D7V8B4SM:existing_process":[],"2D6JWT1U6:existing_process":[],"2D8S4RU7Z:existing_process":[],"2D9YYJMG9:existing_process":[],"2D86WT5A7:existing_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}